{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3+scNu0YaeKHatMF4SJZ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lav162329/product-category-classifier/blob/main/notebooks/02_feature_engineering_and_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is dedicated to the application of advanced **Feature Engineering** techniques and the comparison of multiple Machine Learning algorithms. The primary goal is to create an optimal, robust model for automated product classification.\n",
        "\n",
        "This notebook builds directly upon the data cleansing and exploratory insights gained from `01_data_exploration.ipynb`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "djJ_32Tc-XYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Initialization and Data Loading"
      ],
      "metadata": {
        "id": "zycxo3Oe-gVI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZVq6Afa-EPx",
        "outputId": "97f243c3-f2db-4c89-bd70-04d5eac4a5fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data for modeling loaded and cleaned. Shape: (35096, 8)\n",
            "\n",
            "Ready to proceed with Feature Engineering.\n"
          ]
        }
      ],
      "source": [
        "# Initialization and Data Loading\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "from scipy.sparse import hstack\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') # Suppress warnings for cleaner output\n",
        "\n",
        "# Load the data and perform minimal essential cleaning (reproduction of initial steps)\n",
        "url = \"https://raw.githubusercontent.com/lav162329/product-category-classifier/main/data/products.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Essential cleaning steps\n",
        "df.columns = df.columns.str.strip()\n",
        "df.dropna(subset=['Product Title', 'Category Label'], inplace=True)\n",
        "df['Product Title'] = df['Product Title'].astype(str).str.lower()\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Data for modeling loaded and cleaned. Shape: {df.shape}\")\n",
        "print(\"\\nReady to proceed with Feature Engineering.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 1: Feature Engineering\n",
        "\n",
        "## 2: Detailed Feature Engineering\n",
        "\n",
        "This step involves creating specific numerical features (length, presence of units, etc.). Regular expression patterns are adapted using **non-capturing groups `(?:...)`** to suppress `UserWarning` and ensure compatibility across different Pandas versions. These features provide essential context to the model, significantly improving classification performance.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3m6cbdjsArar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Detailed Feature Engineering.\n",
        "\n",
        "# Define the feature creation function\n",
        "def create_engineered_features(df):\n",
        "\n",
        "    titles = df['Product Title']\n",
        "\n",
        "    # Structural Features\n",
        "    df['title_length'] = titles.apply(len)\n",
        "    df['word_count'] = titles.apply(lambda x: len(x.split()))\n",
        "\n",
        "    # Binary Specification Markers (using non-capturing groups to prevent warnings)\n",
        "\n",
        "    # 1. has_storage_unit: Capacity/Volume (GB, TB, L, KG)\n",
        "    storage_pattern = r'(?:\\d+)\\s*(?:gb|tb|mb|l|kg|litre)\\b'\n",
        "    df['has_storage_unit'] = titles.str.contains(storage_pattern, regex=True).astype(int)\n",
        "\n",
        "    # 2. has_dimension: Size (cm, inch, \")\n",
        "    dimension_pattern = r'(?:\\d+(?:\\.\\d+)?)\\s*(?:cm|inch|\")\\b'\n",
        "    df['has_dimension'] = titles.str.contains(dimension_pattern, regex=True).astype(int)\n",
        "\n",
        "    # 3. has_digit: Presence of any number\n",
        "    df['has_digit'] = titles.str.contains(r'\\d+', regex=True).astype(int)\n",
        "\n",
        "    # 4. is_tech_product: Common tech acronyms\n",
        "    tech_pattern = r'\\b(?:ssd|led|usb|hdmi|wifi|ghz|mp|core)\\b'\n",
        "    df['is_tech_product'] = titles.str.contains(tech_pattern, regex=True).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply and define feature columns\n",
        "df = create_engineered_features(df)\n",
        "FEATURE_COLS = ['title_length', 'word_count', 'has_storage_unit', 'has_dimension', 'has_digit', 'is_tech_product']\n",
        "\n",
        "print(\"Engineered features created successfully.\")\n",
        "print(df[FEATURE_COLS].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR_OLmvqA72R",
        "outputId": "753ee0d1-6392-430f-8008-1b6ce2fd3e00"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Engineered features created successfully.\n",
            "   title_length  word_count  has_storage_unit  has_dimension  has_digit  \\\n",
            "0            31           6                 1              0          1   \n",
            "1            35           7                 1              0          1   \n",
            "2            70          13                 1              0          1   \n",
            "3            35           7                 1              0          1   \n",
            "4            54          11                 1              0          1   \n",
            "\n",
            "   is_tech_product  \n",
            "0                0  \n",
            "1                0  \n",
            "2                0  \n",
            "3                0  \n",
            "4                0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3: Standardization, Vectorization and Data Partitioning\n",
        "\n",
        "Preparation of the final sparse matrix and storage of the transformers (Scaler, Vectorizer, Encoder)."
      ],
      "metadata": {
        "id": "jzDc0Dy3Bq5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization, Vectorization, and Data Split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler # Dodajemo MinMaxScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Path Setup and Directory Creation\n",
        "MODELS_PATH = '../models/'\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "\n",
        "# Lista numeriƒçkih karakteristika (preuzeta iz CELL 2)\n",
        "FEATURE_COLS = ['title_length', 'word_count', 'has_storage_unit', 'has_dimension', 'has_digit', 'is_tech_product']\n",
        "\n",
        "# 2. Standardization of Numerical Features\n",
        "# Koristimo MinMaxScaler da osiguramo nenegativne vrednosti za ComplementNB\n",
        "scaler = MinMaxScaler()\n",
        "df[FEATURE_COLS] = scaler.fit_transform(df[FEATURE_COLS])\n",
        "joblib.dump(scaler, os.path.join(MODELS_PATH, 'scaler.pkl'))\n",
        "print(\"MinMaxScaler (used for compatibility) saved successfully.\")\n",
        "\n",
        "# 3. Encoding the Target Variable\n",
        "le = LabelEncoder()\n",
        "df['Category_Encoded'] = le.fit_transform(df['Category Label'])\n",
        "y = df['Category_Encoded']\n",
        "joblib.dump(le, os.path.join(MODELS_PATH, 'label_encoder.pkl'))\n",
        "print(\"LabelEncoder saved successfully.\")\n",
        "\n",
        "# 4. Text Vectorization (TF-IDF)\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=10000)\n",
        "X_text = tfidf_vectorizer.fit_transform(df['Product Title'])\n",
        "joblib.dump(tfidf_vectorizer, os.path.join(MODELS_PATH, 'tfidf_vectorizer.pkl'))\n",
        "print(\"TF-IDF Vectorizer saved successfully.\")\n",
        "\n",
        "# 5. Combining Features\n",
        "X_numerical = df[FEATURE_COLS].values\n",
        "X_final = hstack([X_text, X_numerical])\n",
        "\n",
        "# 6. Splitting Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nCombined Feature Matrix Shape (X_final): {X_final.shape}\")\n",
        "print(f\"Final Training Set Shape (X_train): {X_train.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtE7QEKgM-dT",
        "outputId": "f9bcad01-a7e0-49c2-b232-b4d597feeb4e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMaxScaler (used for compatibility) saved successfully.\n",
            "LabelEncoder saved successfully.\n",
            "TF-IDF Vectorizer saved successfully.\n",
            "\n",
            "Combined Feature Matrix Shape (X_final): (35096, 10006)\n",
            "Final Training Set Shape (X_train): (28076, 10006)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Phase 2: Modeling and Evaluation\n",
        "\n",
        "4: Comparison of ML Algorithms (4 models)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IPPie1KLLDMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Comparison (Training and Macro F1 Evaluation)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"1. Logistic Regression\": LogisticRegression(solver='saga', max_iter=500, random_state=42, n_jobs=-1, C=1.0),\n",
        "    \"2. Linear SVC\": LinearSVC(random_state=42, C=0.5, max_iter=5000),\n",
        "    \"3. Complement Naive Bayes\": ComplementNB(), # Sada radi zbog MinMaxScaler u CELL 3\n",
        "    \"4. Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(\"--- Starting Model Training and Comparison ---\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"Training: {name}...\")\n",
        "\n",
        "    # Training\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluation Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0, output_dict=True)\n",
        "    f1_macro = report['macro avg']['f1-score']\n",
        "\n",
        "    results[name] = {\"Accuracy\": accuracy, \"F1-Macro\": f1_macro, \"Model\": model, \"y_pred\": y_pred}\n",
        "\n",
        "    print(f\"[{name}] Accuracy: {accuracy:.4f} | Macro F1-Score: {f1_macro:.4f}\")\n",
        "\n",
        "# Summary\n",
        "summary_df = pd.DataFrame({\n",
        "    'Model': [name for name in results.keys()],\n",
        "    'Accuracy': [res['Accuracy'] for res in results.values()],\n",
        "    'Macro F1-Score': [res['F1-Macro'] for res in results.values()]\n",
        "}).sort_values(by='Macro F1-Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(\"\\n========== FINAL MODEL COMPARISON SUMMARY ==========\")\n",
        "print(summary_df.to_markdown(index=False))\n",
        "\n",
        "best_model_name = summary_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['Model']\n",
        "best_model_predictions = results[best_model_name]['y_pred']\n",
        "print(f\"\\nOptimal Model Selected: **{best_model_name}**\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqNFYp31NOum",
        "outputId": "d2250a32-02bf-422e-f027-150d1a21e09b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Model Training and Comparison ---\n",
            "Training: 1. Logistic Regression...\n",
            "[1. Logistic Regression] Accuracy: 0.9544 | Macro F1-Score: 0.7382\n",
            "Training: 2. Linear SVC...\n",
            "[2. Linear SVC] Accuracy: 0.9577 | Macro F1-Score: 0.7405\n",
            "Training: 3. Complement Naive Bayes...\n",
            "[3. Complement Naive Bayes] Accuracy: 0.9474 | Macro F1-Score: 0.7318\n",
            "Training: 4. Random Forest...\n",
            "[4. Random Forest] Accuracy: 0.8732 | Macro F1-Score: 0.6761\n",
            "\n",
            "========== FINAL MODEL COMPARISON SUMMARY ==========\n",
            "| Model                     |   Accuracy |   Macro F1-Score |\n",
            "|:--------------------------|-----------:|-----------------:|\n",
            "| 2. Linear SVC             |   0.957692 |         0.740505 |\n",
            "| 1. Logistic Regression    |   0.954416 |         0.738201 |\n",
            "| 3. Complement Naive Bayes |   0.947436 |         0.731827 |\n",
            "| 4. Random Forest          |   0.873219 |         0.676093 |\n",
            "\n",
            "Optimal Model Selected: **2. Linear SVC**\n"
          ]
        }
      ]
    }
  ]
}